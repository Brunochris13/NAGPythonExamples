{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized multiplication of triangular matrices with the NAG Library for Python\n",
    "\n",
    "In the NAG blog post [Exploting Matrix Structure in the solution of linear systems](https://www.nag.com/content/exploiting-matrix-structure-solution-linear-systems) there is a demonstation showing how paying attention to matrix structure can reap dividends in optimization of computation time.  Using specialised solvers at the right time can significantly speed up your computational workflow.\n",
    "\n",
    "One case that was of interest to one of our clients was matrix-matrix multiplication where **both** input matrices were triangular.  The standard BLAS specification has a routine called **dtrmm** (also implemented in the NAG Library for Python) which contains optimizations when **one** of the input matrices is triangular but there was nothing that took advantage of the situation where **both** input matrices are triangular.\n",
    "\n",
    "Two of the new routines in Mark 27 of the NAG library considers the case of matrix-matrix multiplication when both input matrices are [upper or lower triangular](https://en.wikipedia.org/wiki/Triangular_matrix) in structure. The two new routines, **real_tri_matmul_inplace** and **complex_tri_matmul_inplace** implement real and complex versions of this calculation respectively.  \n",
    "\n",
    "We have attempted to make the new routines as 'BLAS-like' as possible so if you are used to using **dtrmm**, you'll have no problem using the new material. \n",
    "\n",
    "In this demonstration, we show how calling this routine from the [NAG Library for Python](https://www.nag.com/nag-library-python) can result in faster multiplications of large matrices compared to using the default matrix-matrix-multiplication or the more specialised **dtrmm** even when an optimized [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) such as the [Intel MKL](https://software.intel.com/en-us/mkl) is in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use real_tri_matmul_inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple demonstration showing how to use the routine.  Ths is based on the example in the [Fortran Documentation](https://www.nag.com/numeric/nl/nagdoc_latest/flhtml/f01/f01dgf.html) which computes the matrix product \n",
    "\n",
    "$$C = \\alpha A^T B$$\n",
    "\n",
    "where both A and B are upper triangular matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naginterfaces.library.matop import real_tri_matmul_inplace\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [1.5,2.3,6.7,1.9],\n",
    "    [0.0,3.4,5.4,8.6],\n",
    "    [0.0,0.0,8.1,2.0],\n",
    "    [0.0,0.0,0.0,5.9]]\n",
    ")\n",
    "    \n",
    "B = np.array([\n",
    "    [3.5,2.1,4.0,2.1],\n",
    "    [0.0,5.6,2.1,2.5],\n",
    "    [0.0,0.0,1.7,1.1],\n",
    "    [0.0,0.0,0.0,7.4]]\n",
    ")\n",
    "\n",
    "# Is B operated on from the left 'L' or right 'R'\n",
    "side   = 'L'\n",
    "# Are A and B upper triangular 'U' or lower triangular 'L'\n",
    "uplo   = 'U' \n",
    "# Are we dealing with A (no transpose) 'N' or transpose(A) 'T'\n",
    "transa = 'T'\n",
    "# What's the value of the scalar alpha\n",
    "alpha = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_tri_matmul_inplace(side, uplo, transa, alpha, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine is an **inplace** function which means that there is no return value.  Instead, the input argument **B** is modified in-place to contain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1  ,  1.26 ,  2.4  ,  1.26 ],\n",
       "       [ 3.22 ,  9.548,  6.536,  5.332],\n",
       "       [ 9.38 , 17.724, 20.764, 14.592],\n",
       "       [ 2.66 , 20.86 , 11.624, 28.54 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B Now contains the result of the calculation alpha*transpose(A)*B\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard Python equivalent for this calculation is the following although it does not attempt to take advantage of matrix structure at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1  ,  1.26 ,  2.4  ,  1.26 ],\n",
       "       [ 3.22 ,  9.548,  6.536,  5.332],\n",
       "       [ 9.38 , 17.724, 20.764, 14.592],\n",
       "       [ 2.66 , 20.86 , 11.624, 28.54 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1.5,2.3,6.7,1.9],\n",
    "    [0.0,3.4,5.4,8.6],\n",
    "    [0.0,0.0,8.1,2.0],\n",
    "    [0.0,0.0,0.0,5.9]]\n",
    ")\n",
    "    \n",
    "# Redefine B because real_tri_matmul_inplace modified it\n",
    "B = np.array([\n",
    "    [3.5,2.1,4.0,2.1],\n",
    "    [0.0,5.6,2.1,2.5],\n",
    "    [0.0,0.0,1.7,1.1],\n",
    "    [0.0,0.0,0.0,7.4]]\n",
    ")\n",
    "\n",
    "alpha*(np.transpose(A) @ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed benefits for large matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-Matrix multiplication is one of the most important operations in computational science. So much so that CPU and GPU manufacturers dedicate some of their hardware to the optimization of this problem.  NVIDIA developed [Tensor Cores](https://www.nvidia.com/en-gb/data-center/tensorcore/) for example and relatively recent advances in CPU instructions such as [AVX (Advanced Vector Extensions)](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) and [FMA (Fused Multiply-Add)](https://en.wikipedia.org/wiki/FMA_instruction_set) have allowed computers to perform matrix-matrix multiply faster than ever.\n",
    "\n",
    "Given its importance, matrix-matrix multiplication is heavily optimized in modern software libraries.  In this demonstration, a version of Python's Numpy was used that is linked to [Intel's MKL](https://software.intel.com/en-us/mkl) so the standard matrix multiplication operator works as fast as possible on Intel hardware.\n",
    "\n",
    "Given triangular matrices and NAG's new routine, we can now do even better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random upper triangular matrices of size N x N\n",
    "N = 10000\n",
    "# For reproducibility\n",
    "np.random.seed(1)\n",
    "# Construct random upper triangular matrices\n",
    "A = np.triu(np.random.random((N, N)))\n",
    "B = np.triu(np.random.random((N, N)))\n",
    "\n",
    "# Is B operated on from the left 'L' or right 'R'\n",
    "side   = 'L'\n",
    "# Are A and B upper triangular 'U' or lower triangular 'L'\n",
    "uplo   = 'U' \n",
    "# Are we dealing with A (no transpose) 'N' or transpose(A) 'T'\n",
    "transa = 'N'\n",
    "# What's the value of the scalar alpha\n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see how long this takes using the standard Python operator.  This time, I have modifed the **transa** and **alpha** arguments of NAG routine so that we are simply performing a standard matrix-matrix multiply $C=AB$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 s ± 1.3 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit N = 10000;np.random.seed(1);A = np.triu(np.random.random((N,N)));B = np.triu(np.random.random((N,N)))\n",
    "#The line of code above is the set-up for each run of timeit. It is not timed. Only the body below is timed\n",
    "C = A @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLAS routine **dtrmm** includes optimizations for when **one** of the input matrices is triangular.  If the second is also triangular, it won't take advantage of this at all.  It's still useful here though and shows a decent speed-up over the standard matrix-matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naginterfaces.library.blas import dtrmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9 s ± 1.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit N = 10000;np.random.seed(1);A = np.triu(np.random.random((N,N)));B = np.triu(np.random.random((N,N)));\n",
    "C = dtrmm(side='L', uplo='U', transa='N', diag='N', m=N, alpha=alpha, a=A, b=B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try the new NAG routine that has no equivalent in BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.88 s ± 617 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit N = 10000;np.random.seed(1);A = np.triu(np.random.random((N,N)));B = np.triu(np.random.random((N,N)))\n",
    "#The line of code above is the set-up for each run of timeit. It is not timed. Only the body below is timed\n",
    "#The set up is necessary here because each call to real_tri_matmul_inplace modifies the input matrix B\n",
    "real_tri_matmul_inplace(side, uplo, transa, alpha, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLAS and LAPACK details\n",
    "\n",
    "When comparing linear algebra routines, it is very important to give details of the BLAS and LAPACK implementations used along with some system details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkl_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\include']\n",
      "blas_mkl_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\include']\n",
      "blas_opt_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\include']\n",
      "lapack_mkl_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\include']\n",
      "lapack_opt_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/michael.croucher/AppData/Local/Continuum/anaconda3/envs/intel\\\\Library\\\\include']\n"
     ]
    }
   ],
   "source": [
    "# Numpy config details. I used a version of Numpy linked to the Intel MKL and provided by the Intel Python distribution.\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Microsoft Windows x64, 64-bit, Intel C/C++ or Microsoft C/C++ or Intel Fortran',\n",
       " 'integer_bits': 32,\n",
       " 'precision': 'double',\n",
       " 'product_code': 'NLW6I27DE',\n",
       " 'mark': (27, 0, 0),\n",
       " 'vendor_lib': 'MKL 2019.0.2',\n",
       " 'build_id': 188890,\n",
       " 'hardware': 'x86_64',\n",
       " 'os': 'Windows 64-bit',\n",
       " 'compilers': {'C': {'title': 'Intel(R) C++ Intel(R) 64 Compiler for Intel(R) 64, Version 19.0.2.190 Build 20190117',\n",
       "   'flags': '-O3 -QaxCORE-AVX2,AVX -Qfma- -fp:precise -Qfp-speculation:safe -MD -nologo -w /EHsc -Z7'},\n",
       "  'C++': {'title': 'Intel(R) C++ Intel(R) 64 Compiler for Intel(R) 64, Version 19.0.2.190 Build 20190117',\n",
       "   'flags': '-O3 -QaxCORE-AVX2,AVX -Qfma- -fp:precise -Qfp-speculation:safe -MD -nologo -w /EHsc -Z7'},\n",
       "  'Fortran': {'title': 'Intel(R) Visual Fortran Intel(R) 64 Compiler for Intel(R) 64, Version 19.0.2.190 Build 20190117',\n",
       "   'flags': '-O3 -QaxCORE-AVX2,AVX -Qfma- -fp:precise -Qfp-speculation:safe -auto -MD -nologo -w -Z7'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAG Library details\n",
    "from naginterfaces.library.info import impl_details\n",
    "impl_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n",
      "Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\n",
      "Number of Physical CPUs = 4\n",
      "Number of Logical CPUs = 8\n"
     ]
    }
   ],
   "source": [
    "# Operating system and CPU details\n",
    "import platform\n",
    "import psutil\n",
    "print(platform.architecture())\n",
    "print(platform.processor())\n",
    "print(\"Number of Physical CPUs = {0}\".format(psutil.cpu_count(logical=False)))\n",
    "print(\"Number of Logical CPUs = {0}\".format(psutil.cpu_count(logical=True)))\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
